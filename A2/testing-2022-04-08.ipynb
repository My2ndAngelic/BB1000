{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Evan Huynh\"\n",
    "COLLABORATORS = \"GitHub Copilot\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6273326574ec9693a7f67a3679389d2e",
     "grade": false,
     "grade_id": "cell-c374456facd20e46",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Testing\n",
    "\n",
    "Testing is one of the most important components of sustainable software development. It improves code quality, maintainability and lifetime, and saves developer time. In previous labs you have already come across this in the form of assert statements. There are a number of testing libraries to support Python development and they all have in common that they build on the assert statement. In addition they provide better information when errors are found and the facilitate automated testing of large projects.\n",
    "\n",
    "We will use the doctest and the pytest libraries. If pytest is not installed you need to install it with pip. On your computers do this in the virtual environment where you previously installed jupyter\n",
    "\n",
    "~~~\n",
    "$ pip install pytest\n",
    "$ jupyter notebook\n",
    "~~~\n",
    "\n",
    "Testing libraries are typically designed to be used on Python source files while they are not adapted to be used with Jupyter notebooks. To be able to work with this in this lab, we use cell magic commands (%%file) to save cells in ordinary files and then execute pytest on those files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b419f62e162c6a542eeacf8bdb910bba",
     "grade": false,
     "grade_id": "cell-33440c27e09b63e4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import doctest\n",
    "import pytest\n",
    "# This is for jupyter to recognize changes in external files without restarting kernel\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1: add time stamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6b091c7219eb44034283ac9e92858b70",
     "grade": false,
     "grade_id": "cell-0ea01ce4cc34e8ff",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting timestamps.py\n"
     ]
    }
   ],
   "source": [
    "%%file timestamps.py\n",
    "def sum_timestamps(l):\n",
    "    \"\"\"\n",
    "    >>> sum_timestamps(['5:32', '4:48'])\n",
    "    '10:20'\n",
    "    >>> sum_timestamps(['03:10', '01:00'])\n",
    "    '4:10'\n",
    "    >>> sum_timestamps(['2:10', '1:59'])\n",
    "    '4:09'\n",
    "    >>> sum_timestamps(['15:32', '45:48'])\n",
    "    '1:01:20'\n",
    "    >>> sum_timestamps(['6:15:32', '2:45:48'])\n",
    "    '9:01:20'\n",
    "    >>> sum_timestamps(['6:35:32', '2:45:48', '40:10'])\n",
    "    '10:01:30'\n",
    "    >>> sum_timestamps(['14:32', '45:48'])\n",
    "    '1:00:20'\n",
    "    >>> sum_timestamps(['0:10', '0:20'])\n",
    "    '0:30'\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    second = 0\n",
    "    minute = 0\n",
    "    hour = 0\n",
    "    for k in [list(reversed(j)) for j in [str.split(i, ':') for i in l]]:\n",
    "        second = (second + int(k[0]))\n",
    "        if second > 59:\n",
    "            second = second % 60\n",
    "            minute = minute + 1\n",
    "        minute = (minute + int(k[1]))\n",
    "        if minute > 59:\n",
    "            minute = minute % 60\n",
    "            hour = hour + 1\n",
    "        if len(k) > 2:\n",
    "            hour = (hour + int(k[2])) % 60\n",
    "    return ':'.join(filter(None, [str(hour) if hour > 0 else None,\n",
    "                                  str(minute).zfill(2) if hour > 0 else str(minute),\n",
    "                                  str(second).zfill(2)]\n",
    "                           ))\n",
    "    ################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a869f4cd39aaf8cb6b1d3e9635ce59a9",
     "grade": true,
     "grade_id": "cell-210600fd395cba29",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying:\n",
      "    sum_timestamps(['5:32', '4:48'])\n",
      "Expecting:\n",
      "    '10:20'\n",
      "ok\n",
      "Trying:\n",
      "    sum_timestamps(['03:10', '01:00'])\n",
      "Expecting:\n",
      "    '4:10'\n",
      "ok\n",
      "Trying:\n",
      "    sum_timestamps(['2:10', '1:59'])\n",
      "Expecting:\n",
      "    '4:09'\n",
      "ok\n",
      "Trying:\n",
      "    sum_timestamps(['15:32', '45:48'])\n",
      "Expecting:\n",
      "    '1:01:20'\n",
      "ok\n",
      "Trying:\n",
      "    sum_timestamps(['6:15:32', '2:45:48'])\n",
      "Expecting:\n",
      "    '9:01:20'\n",
      "ok\n",
      "Trying:\n",
      "    sum_timestamps(['6:35:32', '2:45:48', '40:10'])\n",
      "Expecting:\n",
      "    '10:01:30'\n",
      "ok\n",
      "Trying:\n",
      "    sum_timestamps(['14:32', '45:48'])\n",
      "Expecting:\n",
      "    '1:00:20'\n",
      "ok\n",
      "Trying:\n",
      "    sum_timestamps(['0:10', '0:20'])\n",
      "Expecting:\n",
      "    '0:30'\n",
      "ok\n",
      "1 items had no tests:\n",
      "    timestamps\n",
      "1 items passed all tests:\n",
      "   8 tests in timestamps.sum_timestamps\n",
      "8 tests in 2 items.\n",
      "8 passed and 0 failed.\n",
      "Test passed.\n"
     ]
    },
    {
     "data": {
      "text/plain": "TestResults(failed=0, attempted=8)"
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timestamps\n",
    "doctest.testmod(timestamps, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Assignment 2: time_stamps with pytest\n",
    "\n",
    "Write a test file to be used with pytest for assignment 1, with the same test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f15f003d3deb585ca2cbcb1bba918eea",
     "grade": false,
     "grade_id": "cell-ed7cd15078053e50",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_timestamps.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_timestamps.py\n",
    "\n",
    "from timestamps import sum_timestamps\n",
    "# YOUR CODE HERE\n",
    "def test_run():\n",
    "    assert sum_timestamps(['5:32', '4:48']) == '10:20'\n",
    "    assert sum_timestamps(['03:10', '01:00']) == '4:10'\n",
    "    assert sum_timestamps(['2:10', '1:59']) == '4:09'\n",
    "    assert sum_timestamps(['15:32', '45:48']) == '1:01:20'\n",
    "    assert sum_timestamps(['6:15:32', '2:45:48']) == '9:01:20'\n",
    "    assert sum_timestamps(['6:35:32', '2:45:48', '40:10']) == '10:01:30'\n",
    "    assert sum_timestamps(['14:32', '45:48']) == '1:00:20'\n",
    "    assert sum_timestamps(['0:10', '0:20']) == '0:30'\n",
    "################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ee29c06d79b8bcedd807baa0cfac3473",
     "grade": true,
     "grade_id": "cell-c597b267d405be1f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.8.10, pytest-7.1.1, pluggy-1.0.0 -- C:\\Users\\My2ndAngelic\\Desktop\\Courses\\BB1000\\venv\\Scripts\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: C:\\Users\\My2ndAngelic\\Desktop\\Courses\\BB1000\\A2\n",
      "collecting ... collected 1 item\n",
      "\n",
      "test_timestamps.py::test_run PASSED                                      [100%]\n",
      "\n",
      "============================== 1 passed in 0.01s ==============================\n"
     ]
    },
    {
     "data": {
      "text/plain": "<ExitCode.OK: 0>"
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytest.main(['-v', 'test_timestamps.py'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aff4a31a576f7a13a7f174e005cf620b",
     "grade": false,
     "grade_id": "cell-a4c4bdbc6f98dac1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment 3: running mean\n",
    "\n",
    "This is an example use of parametrized test cases. Look at the test function and understand how it works.\n",
    "Write a function that passes the tests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f15c3812d0a4c64c65dfe19947283f0b",
     "grade": false,
     "grade_id": "cell-a62f5af5275891d9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_running_mean.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_running_mean.py\n",
    "import pytest\n",
    "\n",
    "from running_mean import running_mean\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\"input_argument, expected_return\", [\n",
    "    ([1, 2, 3], [1, 1.5, 2]),\n",
    "    ([2, 6, 10, 8, 11, 10],\n",
    "     [2.0, 4.0, 6.0, 6.5, 7.4, 7.83]),\n",
    "    ([3, 4, 6, 2, 1, 9, 0, 7, 5, 8],\n",
    "     [3.0, 3.5, 4.33, 3.75, 3.2, 4.17, 3.57, 4.0, 4.11, 4.5]),\n",
    "    ([], []),\n",
    "])\n",
    "def test_running_mean(input_argument, expected_return):\n",
    "    ret = list(running_mean(input_argument))\n",
    "    assert ret == expected_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "772c3fe89217aea5bc44c3753181dbbe",
     "grade": false,
     "grade_id": "cell-96c10ee58ecac500",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting running_mean.py\n"
     ]
    }
   ],
   "source": [
    "%%file running_mean.py\n",
    "def running_mean(sequence):\n",
    "    \"\"\"Calculate the running mean of the sequence passed in,\n",
    "       returns a sequence of same length with the averages.\n",
    "       You can assume all items in sequence are numeric.\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return [round(sum(sequence[:i+1])/(i+1), 2) for i, _ in enumerate(sequence)]\n",
    "    ################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2214c633a7a294cb33de7bbac7d2bad5",
     "grade": true,
     "grade_id": "cell-6a057ac659530fd9",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.8.10, pytest-7.1.1, pluggy-1.0.0 -- C:\\Users\\My2ndAngelic\\Desktop\\Courses\\BB1000\\venv\\Scripts\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: C:\\Users\\My2ndAngelic\\Desktop\\Courses\\BB1000\\A2\n",
      "collecting ... collected 4 items\n",
      "\n",
      "test_running_mean.py::test_running_mean[input_argument0-expected_return0] PASSED [ 25%]\n",
      "test_running_mean.py::test_running_mean[input_argument1-expected_return1] PASSED [ 50%]\n",
      "test_running_mean.py::test_running_mean[input_argument2-expected_return2] PASSED [ 75%]\n",
      "test_running_mean.py::test_running_mean[input_argument3-expected_return3] PASSED [100%]\n",
      "\n",
      "============================== 4 passed in 0.02s ==============================\n"
     ]
    },
    {
     "data": {
      "text/plain": "<ExitCode.OK: 0>"
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytest.main(['-v', 'test_running_mean.py'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 4\n",
    "\n",
    "Write a second version of the test function for timestamp where the different test cases are different parameterizations for one test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ad40885363cb8747ef38c820b6420f30",
     "grade": false,
     "grade_id": "cell-1d068aaf56835af8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_timestamps_2.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_timestamps_2.py\n",
    "import pytest\n",
    "from timestamps import sum_timestamps\n",
    "\n",
    "# YOUR CODE HERE\n",
    "@pytest.mark.parametrize(\"input_argument, expected_return\", [\n",
    "    (['5:32', '4:48'] , \"10:20\"),\n",
    "    (['03:10', '01:00'] , \"4:10\"),\n",
    "    (['2:10', '1:59'] , \"4:09\"),\n",
    "    (['15:32', '45:48'] , \"1:01:20\"),\n",
    "    (['6:15:32', '2:45:48'] , \"9:01:20\"),\n",
    "    (['6:35:32', '2:45:48', '40:10'] , \"10:01:30\"),\n",
    "    (['14:32', '45:48'] , \"1:00:20\"),\n",
    "    (['0:10', '0:20'] , \"0:30\"),\n",
    "])\n",
    "def test_timestamps_2(input_argument, expected_return):\n",
    "    ret = sum_timestamps(input_argument)\n",
    "    assert ret == expected_return\n",
    "################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "77cf3d292ff9c2ea8171ce5f579ccad6",
     "grade": true,
     "grade_id": "cell-32b906546d801ca5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.8.10, pytest-7.1.1, pluggy-1.0.0 -- C:\\Users\\My2ndAngelic\\Desktop\\Courses\\BB1000\\venv\\Scripts\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: C:\\Users\\My2ndAngelic\\Desktop\\Courses\\BB1000\\A2\n",
      "collecting ... collected 8 items\n",
      "\n",
      "test_timestamps_2.py::test_timestamps_2[input_argument0-10:20] PASSED    [ 12%]\n",
      "test_timestamps_2.py::test_timestamps_2[input_argument1-4:10] PASSED     [ 25%]\n",
      "test_timestamps_2.py::test_timestamps_2[input_argument2-4:09] PASSED     [ 37%]\n",
      "test_timestamps_2.py::test_timestamps_2[input_argument3-1:01:20] PASSED  [ 50%]\n",
      "test_timestamps_2.py::test_timestamps_2[input_argument4-9:01:20] PASSED  [ 62%]\n",
      "test_timestamps_2.py::test_timestamps_2[input_argument5-10:01:30] PASSED [ 75%]\n",
      "test_timestamps_2.py::test_timestamps_2[input_argument6-1:00:20] PASSED  [ 87%]\n",
      "test_timestamps_2.py::test_timestamps_2[input_argument7-0:30] PASSED     [100%]\n",
      "\n",
      "============================== 8 passed in 0.04s ==============================\n"
     ]
    },
    {
     "data": {
      "text/plain": "<ExitCode.OK: 0>"
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytest.main(['-v', 'test_timestamps_2.py'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}